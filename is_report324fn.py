# -*- coding: utf-8 -*-
"""IS_Report324_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KEDsfszS21KtMhjqmCrri3X4SKZGkkRF
"""

import pandas as pd
import numpy as np
import joblib as joblib

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.cm

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB

import re
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier

"""# New Section"""

data = pd.read_csv('phishing_urls.csv')
data

data = loaddata()

data.head()

data

print('No. of rows = ',data.shape[0])
print('No. of columns =',data.shape[1])

data.info()



"""Data Cleansing"""

def clean_data(data):
    data = data.drop_duplicates(keep='first')
    return data

data = clean_data(data)
print('data delete duplicate: ',data.shape[0])

data.describe(include='all')



"""Feature


```
## This is formatted as code
```

# New Section
"""

data_extract = data.copy()

tokenizer = RegexpTokenizer(r'[A-Za-z]+')
def splitUrlIntoToken(url):
    return tokenizer.tokenize(url)

data_extract['split_url_list'] = data_extract['URL'].map(lambda t: splitUrlIntoToken(t))
print(data_extract['split_url_list'])
data_extract



def removeTextFromList(input, rmTxt):
    input = [x for x in input if (x not in rmTxt)]
    return input

removeTextList = ['www','http','https']
data_extract['split_url_list'] = data_extract['split_url_list'].map(lambda t: removeTextFromList(t, removeTextList))
data_extract

snowball = SnowballStemmer("english")

def textNormByStemType(inputTxt, stemtype):
    inputTxt =[stemtype.stem(word) for word in inputTxt]
    return inputTxt

data_extract['split_url_list'] = data_extract['split_url_list'].map(lambda t: textNormByStemType(t, snowball))
data_extract

data_extract['split_url_text'] = data_extract['split_url_list'].map(lambda t: ' '.join(t))
print(data_extract['split_url_text'][10])

"""# New Section"""

tfid = TfidfVectorizer()

def convertTextToVectorByType(data, vectorType):
    return vectorType.fit_transform(data['split_url_text'])

    
    # return vectorType.fit_transform(data)

tfidText = convertTextToVectorByType(data_extract, tfid)

print(tfidText)

def extract_feature(data, vectorType):

    label = data['Label']
    
    # Split url function
    data['split_url_list'] = data['URL'].map(lambda t: splitUrlIntoToken(t))
    # print(data['split_url_list'])
    # Remove text function
    removeTextList = ['www','http','https']
    data['split_url_list'] = data['split_url_list'].map(lambda t: removeTextFromList(t, removeTextList))
    # print(data['split_url_list'])
    # Stemming function
    snowball = SnowballStemmer("english")
    data['split_url_list'] = data['split_url_list'].map(lambda t: textNormByStemType(t, snowball))
    # print(data['split_url_list'])
    # join text
    data['split_url_text'] = data['split_url_list'].map(lambda t: ' '.join(t))
    
    # Convert to vector function
    vectorText = convertTextToVectorByType(data, vectorType)
    # print("vectorText", vectorText.shape)
    return vectorText, label

data_feature, data_label = extract_feature(data, tfid)

"""Train Test

"""

def data_spliting(data_feature,data_label):
  x_train, x_test, y_train, y_test = train_test_split(data_feature, data_label, test_size=0.2, random_state = 22)
  return x_train, x_test, y_train, y_test

x_train, x_test, y_train, y_test  = data_spliting(data_feature, data_label)

print(x_test[10])

x_train.shape, y_train.shape
print(x_train.shape)

x_test.shape, y_test.shape





"""Train Cross Validate"""

logisticregress = LogisticRegression()
nb = MultinomialNB()
knn = KNeighborsClassifier(n_neighbors=2)
detree = DecisionTreeClassifier()

def trainByModelType(x_train, y_train, modelType):
  cv = KFold(n_splits = 10, random_state = 22, shuffle = True)
  model = modelType
  scores = cross_val_score(model, x_train, y_train, scoring = 'accuracy', cv = cv, n_jobs = -1)
  return scores

logisregress_cv_score = trainByModelType(x_train,y_train, logisticregress)
logisregress_cv_score.mean()

logisregress_cv_score.mean()

"""
pipeline"""

def pipe_model(vectorType, modelType):
    # Getdata
    data = loaddata()
    # CleanData
    data = clean_data(data)
    # ExtractFeature
    data_feature, data_label = extract_feature(data, vectorType)
    joblib.dump(tfid, 'vectorizer.pkl')
    # Train and Test Data
    x_train, x_test, y_train, y_test = data_spliting(data_feature, data_label)
    # Train model with 10-fold cross validation
    modelAccuracy = trainByModelType(x_train,y_train, modelType)
    meanModelAccuracy = modelAccuracy.mean()
    return meanModelAccuracy

print(pipe_model(tfid, logisticregress))

print(pipe_model(tfid, nb))

print(pipe_model(tfid, detree))

print(pipe_model(tfid, knn))

"""# New Section"""

clf = LogisticRegression()
clf.fit(x_train,y_train)
model = clf

y_predict = model.predict(x_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_predict))

target = pd.DataFrame({
        "URL":pd.Series(['www.gooogle.com']),
        "Label":''
    })

target, data_label = extract_feature(target,tfidText)
print(target)

y_predict = model.predict(y_test)
print(y_test)

print(x_test[10].shape)

clf = MultinomialNB()
clf.fit(x_train,y_train)

y_predict = model.predict(x_test[10])
print(y_predict)
print(x_test[10])
# print("Accuracy:",metrics.accuracy_score(y_test, y_predict))

clf = LogisticRegression()
clf.fit(x_train,y_train)

y_predict = model.predict(x_test[10])
# print("Accuracy:",metrics.accuracy_score(y_test, y_predict))

joblib.dump(model,"model2.joblib",compress = 9)

"""Confuision metric"""

confusionMatrix = pd.DataFrame(confusion_matrix(y_predict, y_test),columns = ['Predicted:Bad', 'Predicted:Good'],index = ['Real:Bad', 'Real:Good'])
plt.figure(figsize= (6,4))
sns.heatmap(confusionMatrix, annot = True,fmt='g', cmap="Greens", annot_kws = {'size': 14})

print(classification_report(y_predict, y_test, target_names =['Bad','Good']))

def project():
    targetURL = request.json['URL']
    
    target = pd.DataFrame({
        "URL":pd.Series([targetURL]),
        "Label":''
    })

    predict_data = extract_feature(target, tfid)
    print(predict_data)
    print(type(predict_data), '<============================================================')
    print(predict_data.shape, '<============================================================')
    # y_predict = model.predict(predict_data[0])
    
    
    # return y_predict.tolist()
    return "test"

tet = project()

